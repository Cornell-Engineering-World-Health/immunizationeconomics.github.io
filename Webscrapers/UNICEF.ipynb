{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webscraping complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For this webscraper, the job description and requirements were combined into the same cell because they were not \n",
    "easily separated from the site's html. \n",
    "For future look at separation, a strategy could be to create a list containing strings of the bolded text \n",
    "(found with <strong> in body) and compare this against the entirety of the description and requirements text\n",
    "(description_data.getText()) to section off each headline and concatenate sections according to whether or not\n",
    "they are contained within a word bank such as [\"Background\", \"BACKGROUND\", \"Purpose\"]\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import html\n",
    " \n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://jobs.unicef.org/en-us/listing/'\n",
    "\n",
    "#using headers because originally it wasn't letting us access any of the soup - adding headers allowed us to access information on the website\n",
    "#you may need to do this for some of the medium webscrapers!\n",
    "#If you need help accessing headers lmk\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "}\n",
    "\n",
    "#requesting a session in the specific url\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url, headers = headers)\n",
    "    src = r.content\n",
    "    soup = bs(src)\n",
    "    soup.prettify\n",
    "    \n",
    "    #in UNICEF we have to click a button in order to load all of the job postings \n",
    "    button_count = soup.find('span', {'class':'count'})\n",
    "    \n",
    "    count = int(button_count.text) + 20\n",
    "    new_url = url + \"?page=1&page-items=\" + str(count)\n",
    "    \n",
    "    #getting the soup for the new job postings loaded\n",
    "    r2 = s.get(new_url, headers=headers)\n",
    "    src2 = r2.content\n",
    "    soup2 = bs(src2)\n",
    "    soup2.prettify\n",
    "      \n",
    "    #navigating the page and finding the job posting urls and appending it to allPages\n",
    "    allPages = []\n",
    "    data = soup2.find_all('a', {'class':'job-link'})\n",
    "    for link in data:\n",
    "        url = link.get('href')\n",
    "        #only appending the links that aren't already in allPages\n",
    "        if url not in allPages:\n",
    "            allPages.append(url)\n",
    "            \n",
    "    page_url = [] \n",
    "    job_position = [] \n",
    "    des_and_req = [] \n",
    "    location = [] \n",
    "    ImmEcs = []\n",
    "    organization = []\n",
    "    \n",
    "    #need the root url because href only has the second part of the url\n",
    "    root_url= 'https://jobs.unicef.org'\n",
    "    \n",
    "    #looping through each page in allPages \n",
    "    for page in allPages:\n",
    "        result = s.get(root_url + page)\n",
    "        page_source = result.content\n",
    "        soup = bs(page_source) \n",
    "        soup.prettify\n",
    "        \n",
    "        #stripping all of the content from each page and searching for the key words \n",
    "        for script in soup(['script','style']):\n",
    "            script.decompose()\n",
    "        strips = list(soup.stripped_strings)\n",
    "        strips = str(strips)\n",
    "\n",
    "        immunization = ['Immunization', 'immunisation', 'vaccine', 'vaccines','vaccine-preventable diseases', 'vpd outbreak',\n",
    "            'immunization campaign', 'SIA','supplemental immunization act ivities', 'cold chain', 'GAVI','shigella', 'cholera',\n",
    "            'bcg', 'dtp', 'dpt', 'measles', 'influenza', 'conjugate vaccine']\n",
    "\n",
    "        economics = ['Economics','expenditure tracking', 'financing', \n",
    "            'value for vaccination' , 'costing', 'economic analysis','costs' , 'equity', 'cost effectiveness', 'cost-effectiveness', \n",
    "            'cost benefit analysis', 'benefit-cost analysis','cost utility analysis','budget impact analysis' , 'budget' , 'budgeting' , \n",
    "            'GAVI','funding gap','fiscal']\n",
    "\n",
    "        #checking for Immunization and Economic key words\n",
    "        imm_result = any(ele in strips for ele in immunization)\n",
    "        ec_result = any(ele in strips for ele in economics)\n",
    "\n",
    "         #only enter the if statement if the page has an Immunization or Economics key word\n",
    "        if imm_result or ec_result:\n",
    "            #appending both/immunziation/economics to the column ImmEcs\n",
    "            if (imm_result and ec_result): ImmEcs.append('Both')\n",
    "            elif imm_result: ImmEcs.append('Immunization')\n",
    "            else: ImmEcs.append('Economics')\n",
    "\n",
    "            #appending page url\n",
    "            page_url.append(root_url + page)\n",
    "            \n",
    "            #appending job position name\n",
    "            job_data = soup.find('div', id = 'job-content')\n",
    "            job_position.append(job_data.h2.getText())\n",
    "            \n",
    "            #appending description and requirements data together \n",
    "            #again, having trouble separating the two\n",
    "            description_data = soup.find('div', id = 'job-details')\n",
    "            str_description_data = description_data.getText()\n",
    "            des_and_req.append(str_description_data)\n",
    "\n",
    "            #appending location\n",
    "            location_data = soup.find('span', {'class': 'location'})\n",
    "            location.append(location_data.getText())\n",
    "            \n",
    "            #appending organization\n",
    "            organization.append('UNICEF')\n",
    "\n",
    "DataFrame = pd.DataFrame() \n",
    "DataFrame['Page Url']= page_url \n",
    "DataFrame['Job']= job_position \n",
    "DataFrame['Location'] = location\n",
    "DataFrame['Type'] = ImmEcs\n",
    "DataFrame['Description'] = des_and_req\n",
    "DataFrame['Organization'] = organization\n",
    "\n",
    "\n",
    "Data = DataFrame.drop_duplicates() \n",
    "Data.to_csv(\"UNICEF_Data.csv\")\n",
    "\n",
    "print('Webscraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
