{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webscraping complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For this webscraper, the job description and requirements were combined into the same cell because they were not \n",
    "easily separated from the site's html. \n",
    "For future look at separation, a strategy could be to create a list containing strings of the bolded text \n",
    "(found with <strong> in body) and compare this against the entirety of the description and requirements text\n",
    "(description_data.getText()) to section off each headline and concatenate sections according to whether or not\n",
    "they are contained within a word bank such as [\"Background\", \"BACKGROUND\", \"Purpose\"]\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import html\n",
    " \n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://jobs.unicef.org/en-us/listing/'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url, headers = headers)\n",
    "    src = r.content\n",
    "    soup = bs(src)\n",
    "    soup.prettify\n",
    "    \n",
    "    button_count = soup.find('span', {'class':'count'})\n",
    "    \n",
    "    count = int(button_count.text) + 20\n",
    "    new_url = url + \"?page=1&page-items=\" + str(count)\n",
    "    \n",
    "    r2 = s.get(new_url, headers=headers)\n",
    "    src2 = r2.content\n",
    "    soup2 = bs(src2)\n",
    "    soup2.prettify\n",
    "      \n",
    "    allPages = []\n",
    "    data = soup2.find_all('a', {'class':'job-link'})\n",
    "    for link in data:\n",
    "        url = link.get('href')\n",
    "        if url not in allPages:\n",
    "            allPages.append(url)\n",
    "    #print('Searching pages:')\n",
    "    #print(allPages)\n",
    "    \n",
    "    page_url = [] \n",
    "    job_position = [] \n",
    "    des_and_req = [] \n",
    "    location = [] \n",
    "    Immunization = []\n",
    "    Economics = []\n",
    "    \n",
    "    root_url= 'https://jobs.unicef.org'\n",
    "    \n",
    "    for page in allPages:\n",
    "        result = s.get(root_url + page)\n",
    "        page_source = result.content\n",
    "        soup = bs(page_source) \n",
    "        soup.prettify\n",
    "        \n",
    "        for script in soup(['script','style']):\n",
    "            script.decompose()\n",
    "\n",
    "        strips = list(soup.stripped_strings)\n",
    "        strips = str(strips)\n",
    "\n",
    "        immunization = ['Immunization', 'immunisation', 'vaccine', 'vaccines','vaccine-preventable diseases', 'vpd outbreak',\n",
    "            'immunization campaign', 'SIA','supplemental immunization act ivities', 'cold chain', 'GAVI','shigella', 'cholera',\n",
    "            'bcg', 'dtp', 'dpt', 'measles', 'influenza', 'conjugate vaccine']\n",
    "\n",
    "        economics = ['Economics','expenditure tracking', 'financing', \n",
    "            'value for vaccination' , 'costing', 'economic analysis','costs' , 'equity', 'cost effectiveness', 'cost-effectiveness', \n",
    "            'cost benefit analysis', 'benefit-cost analysis','cost utility analysis','budget impact analysis' , 'budget' , 'budgeting' , \n",
    "            'GAVI','funding gap','fiscal']\n",
    "\n",
    "        imm_result = any(ele in strips for ele in immunization)\n",
    "        ec_result = any(ele in strips for ele in economics)\n",
    "\n",
    "        if imm_result or ec_result:\n",
    "            if imm_result: Immunization.append('True')\n",
    "            else: Immunization.append('False')\n",
    "            if ec_result: Economics.append('True')\n",
    "            else: Economics.append('False')\n",
    "\n",
    "            page_url.append(root_url + page)\n",
    "            \n",
    "            job_data = soup.find('div', id = 'job-content')\n",
    "            job_position.append(job_data.h2.getText())\n",
    "            \n",
    "            description_data = soup.find('div', id = 'job-details')\n",
    "            str_description_data = description_data.getText()\n",
    "            des_and_req.append(str_description_data)\n",
    "\n",
    "            location_data = soup.find('span', {'class': 'location'})\n",
    "            location.append(location_data.getText())\n",
    "            #print(location)\n",
    "\n",
    "DataFrame = pd.DataFrame() \n",
    "DataFrame['Page Url']= page_url \n",
    "DataFrame['Job']= job_position \n",
    "DataFrame['Description and Requirements'] = des_and_req\n",
    "DataFrame['Location'] = location\n",
    "DataFrame['Immunization'] = Immunization\n",
    "DataFrame['Economics'] = Economics\n",
    "\n",
    "Data = DataFrame.drop_duplicates() \n",
    "Data.to_csv(\"UNICEF_Data.csv\")\n",
    "\n",
    "print('Webscraping complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
