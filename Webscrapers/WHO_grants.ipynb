{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "WHO_grants.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX7hI5GCnueI",
        "outputId": "b66301a3-4ced-41bd-9a9d-c9ad5169e5a4"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup as bs \n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://www.who.int/tdr/grants/en/'\n",
        "\n",
        "# define data columns\n",
        "titles = []\n",
        "providers = []\n",
        "deadlines = []\n",
        "descriptions = []\n",
        "\n",
        "# clean up special characters\n",
        "def clean(s):\n",
        "  return re.sub('[^A-Za-z0-9 /-]+', '', s)\n",
        "\n",
        "# requesting a session in the specific url\n",
        "with requests.Session() as s: \n",
        "    r = s.get(url)\n",
        "    src = r.content \n",
        "    soup = bs(src)\n",
        "    soup.prettify()\n",
        "    \n",
        "    # strip contents\n",
        "    for script in soup(['script','style']): \n",
        "        script.decompose()\n",
        "    \n",
        "    #finding grants\n",
        "    grants_list = soup.find_all('div', attrs={'class':'inlay_color'})\n",
        "    grants_list.pop() # remove irrelevant block\n",
        "    #print( grants_list[0])\n",
        "\n",
        "    \n",
        "    for data in grants_list:\n",
        "      soup = bs(str(data))\n",
        "      \n",
        "      text = str(soup.find('span'))\n",
        "      # print(text)\n",
        "      s_title = text.find('<span> ') + len('<span> ')\n",
        "      e_title = text.find('<b>')\n",
        "      s_ddl = text.find('<b>')+len('<b>')\n",
        "      e_ddl = text.find('</b>')\n",
        "\n",
        "      if s_title != -1 and e_title != -1 and s_ddl != -1 and e_ddl != -1: # layout type 1\n",
        "        titles.append(clean(text[s_title:e_title]))\n",
        "        deadlines.append(clean(text[s_ddl:e_ddl]))\n",
        "        providers.append(soup.h4.string)\n",
        "        link = 'https://www.who.int' + soup.li.a.attrs['href']\n",
        "        descriptions.append(link)\n",
        "\n",
        "      else: # layout type 2\n",
        "        tlist = soup.find_all('h4')\n",
        "        dlist = soup.find_all('h5')\n",
        "        plist = soup.find_all('p')\n",
        "        if len(tlist) > 1:\n",
        "          for t,d,p in zip(tlist,dlist,plist):\n",
        "            titles.append(t.string)\n",
        "            deadlines.append(d.string)\n",
        "            providers.append('')\n",
        "            descriptions.append(p.string)\n",
        "\n",
        "DataFrame = pd.DataFrame() \n",
        "DataFrame['Title']= titles \n",
        "DataFrame['Deadline']= deadlines\n",
        "DataFrame['Provider'] = providers\n",
        "DataFrame['Description'] = descriptions\n",
        "\n",
        "Data = DataFrame.drop_duplicates() \n",
        "\n",
        "Data.to_csv(\"WHO_grants.csv\")\n",
        "\n",
        "print('Webscraping complete')\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Webscraping complete\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}