{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/careersection/2020PRD.4.2.21.3.0/css/jquery-ui.css', '/careersection/2020PRD.4.2.21.3.0/css/faceted_search.css', '/careersection/2020PRD.4.2.21.3.0/css/quick_filter_panel.css', '/careersection/2020PRD.4.2.21.3.0/css/result_list_panel.css', '/careersection/2020PRD.4.2.21.3.0/css/header_panel.css', '/careersection/2020PRD.4.2.21.3.0/css/advanced-search-panel.css', '/careersection/2020PRD.4.2.21.3.0/css/see_all_olf_popup.css', '/careersection/2020PRD.4.2.21.3.0/css/footer_panel.css', '/careersection/2020PRD.4.2.21.3.0/UIStyleSheet.dcss?styleSheet=FSStandard_Blue_Arctic&timestamp=1616106606982', '/careersection/theme/13861346/1137362613828/en/theme/css/styles.css', '/careersection/theme/13861346/1137362613828/en/theme/css/faceted_search_theme.css', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '/careersection/ex/mysearches.ftl', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#']\n",
      "Webscraping complete\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.request import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://careers.who.int/careersection/ex/jobsearch.ftl\"\n",
    "\n",
    "#def get_all_website_links(url):\n",
    "   # \"\"\"\n",
    "   # Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "   # \"\"\"\n",
    "    # all URLs of `url`\n",
    "    #urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    #domain_name = urlparse(url).netloc\n",
    "    #soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "    #for a_tag in soup.findAll(\"a\"):\n",
    "        #href = a_tag.attrs.get(\"href\")\n",
    "        #if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "           # continue\n",
    "        #return urls\n",
    "\n",
    "html = urlopen(\"https://careers.who.int/careersection/ex/jobsearch.ftl\")\n",
    "text = html.read()\n",
    "plaintext = text.decode('utf8')\n",
    "allPages = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", plaintext)\n",
    "allPages.remove('#')\n",
    "print(allPages)\n",
    "\n",
    "#data = soup.find_all('div', )\n",
    "#for link in data:\n",
    "    #print(link) \n",
    "    #url = link.find('a')\n",
    "    #print(url)\n",
    "   # if url not in allPages:\n",
    "        #allPages.append(url.get('href'))\n",
    "#print('Searching pages:')\n",
    "#print(allPages)\n",
    "\n",
    "page_url = [] \n",
    "job_position = [] \n",
    "description = [] \n",
    "location = [] \n",
    "job_department = []\n",
    "requirements = []\n",
    "Immunization = []\n",
    "Economics = []\n",
    "\n",
    "root_url= 'https://careers.who.int/'\n",
    "\n",
    "for page in allPages:\n",
    "    result = s.get(root_url + page) \n",
    "    page_source = result.content \n",
    "    soup = bs(page_source) \n",
    "    soup.preetify\n",
    "\n",
    "    for script in soup(['script','style']): \n",
    "        script.decompose()\n",
    "        strips = list(soup.stripped_strings)\n",
    "        strips = str(strips)\n",
    "        #print(strips)\n",
    "\n",
    "        keywords = ['Immunization', 'immunisation', 'vaccine', 'vaccines','vaccine-preventable diseases', 'vpd outbreak',\n",
    "            'immunization campaign', 'SIA','supplemental immunization act ivities', 'cold chain', 'GAVI','shigella', 'cholera',\n",
    "            'bcg', 'dtp', 'dpt', 'measles', 'influenza', 'conjugate vaccine', 'Economics','expenditure tracking', 'financing', \n",
    "            'value for vaccination' , 'costing', 'economic analysis','costs' , 'equity', 'cost effectiveness', 'cost-effectiveness', \n",
    "            'cost benefit analysis', 'benefit-cost analysis','cost utility analysis','budget impact analysis' , 'budget' , 'budgeting' , \n",
    "            'GAVI','funding gap','fiscal']\n",
    "\n",
    "        immunization = ['Immunization', 'immunisation', 'vaccine', 'vaccines','vaccine-preventable diseases', 'vpd outbreak',\n",
    "            'immunization campaign', 'SIA','supplemental immunization act ivities', 'cold chain', 'GAVI','shigella', 'cholera',\n",
    "            'bcg', 'dtp', 'dpt', 'measles', 'influenza', 'conjugate vaccine']\n",
    "\n",
    "        economics = ['Economics','expenditure tracking', 'financing', \n",
    "            'value for vaccination' , 'costing', 'economic analysis','costs' , 'equity', 'cost effectiveness', 'cost-effectiveness', \n",
    "            'cost benefit analysis', 'benefit-cost analysis','cost utility analysis','budget impact analysis' , 'budget' , 'budgeting' , \n",
    "            'GAVI','funding gap','fiscal']\n",
    "\n",
    "        result = any(ele in strips for ele in keywords)\n",
    "        #print (result) \n",
    "\n",
    "        if result == True:\n",
    "\n",
    "            if (any(ele in strips for ele in immunization) == True) and (any(ele in strips for ele in economics)== False):\n",
    "                Immunization.append('True')\n",
    "                Economics.append('False')\n",
    "\n",
    "            if (any(ele in strips for ele in immunization) == False) and (any(ele in strips for ele in economics)== True):\n",
    "                Immunization.append('False')\n",
    "                Economics.append('True')\n",
    "\n",
    "            if (any(ele in strips for ele in immunization) == True) and (any(ele in strips for ele in economics)== True):\n",
    "                Immunization.append('True')\n",
    "                Economics.append('True')\n",
    "\n",
    "            if (root_url+page)=='https://careers.who.int/':\n",
    "                page_url = page_url\n",
    "            else: \n",
    "                page_url.append(root_url+page)\n",
    "            #print(page_url)\n",
    "\n",
    "            job_data = soup.find_all('div', id = 'requisitionDescriptionInterface.ID1347.row1')\n",
    "            for data in job_data:\n",
    "                span = data.find('span', id = 'requisitionDescriptionInterface.reqTitleLinkAction.row1' )\n",
    "                title = span.find('title')\n",
    "                job_title.append(title)\n",
    "                #may need to use get instead\n",
    "            #print(job_title)\n",
    "\n",
    "            description_data = soup.find_all('p', style = 'font-family:Arial')\n",
    "            for data in description_data:\n",
    "                string = str(data)\n",
    "                #print(string)\n",
    "                #cut the string and grab all the text you need\n",
    "                #drop all miscellaneous characters \n",
    "                description.append(des)\n",
    "            #print(description)\n",
    "\n",
    "            requirements_data = soup.find_all('p', style = 'font-family:Arial')\n",
    "            for data in requirements_data:\n",
    "                string = str(data)\n",
    "                #print string\n",
    "                #cut the string and grab all the text you need\n",
    "                #drop all miscellaneous characters \n",
    "                requirements.append(res)\n",
    "            #print(requirements)\n",
    "\n",
    "            location_data = soup.find_all('span', id = 'requisitionDescriptionInterface.ID1692.row1')\n",
    "            for data in location_data:\n",
    "                loc = data.get('title')\n",
    "                location.append(loc)\n",
    "            #print(location)\n",
    "\n",
    "DataFrame = pd.DataFrame() \n",
    "DataFrame['Page Url']= page_url \n",
    "DataFrame['Job']= job_position \n",
    "DataFrame['Description'] = description \n",
    "DataFrame['Requirements']= requirements \n",
    "DataFrame['Location'] = location\n",
    "DataFrame['Immunization'] = Immunization\n",
    "DataFrame['Economics'] = Economics\n",
    "\n",
    "Data = DataFrame.drop_duplicates() \n",
    "Data.to_csv(\"WHO_Data.csv\")\n",
    "\n",
    "print('Webscraping complete')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
